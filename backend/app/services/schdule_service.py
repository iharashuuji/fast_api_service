"""
LLMã«æ¨è«–ã‚’ã•ã›ã¦ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æœ€é©åŒ–ã‚’è¡Œã†
å¿…è¦ãªã‚‚ã®ã¯ã€Openai apiã¨ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å–å¾—ã™ã‚‹äº‹ã€ãã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ãã®äººãŒã©ã‚Œãã‚‰ã„ã®ã‚¿ã‚¹ã‚¯ã‚’ã†ã¾ãã•ã°ã‘ã‚‹ã‹ã‚’è€ƒãˆã‚‹ã€‚
ãŸã ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æœ€é©åŒ–ã£ã¦ã€é›£ã—ã„ã‹ãªã¨æ€ã†ãŒã€é–‹ã„ã¦ã„ã‚‹æ™‚é–“ã§ã¡ã‚‡ã£ã¨ã—ãŸèª²é¡Œã¨ã‹ã¯ã‚„ã‚Œã°ã„ã„ãŒã€ãƒ†ã‚¹ãƒˆå‹‰å¼·ï¼ã¨ã‹æ™‚é–“ã‚’ã¾ã¨ã‚ã¦å–ã£ãŸæ–¹ãŒã„ã„ã‚„ã¤ã¨ã‹ã¯ã‹ãªã‚Šã„ã„ã‹ãªã¨æ€ã£ã¦ã„ã‚‹ã€
ãã“ã§ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ä¸­ã§ã€å‹•ãã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã€å›ºå®šã—ã¦æ±ºã‚ã‚‹ã¹ãã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è€ƒãˆã‚‹ã¹ãã‹ãªã¨æ€ã£ãŸã€‚ã“ã®ã‚ãŸã‚Šã®æ•°å€¤ã®èª¿ç¯€ã¯è‡ªåˆ†è‡ªèº«ã§ã‚„ã£ã¦ã‚‚ã‚‰ã†æ–¹ãŒã„ã„ã‹ãªã¨æ€ã†

    ä½œæˆæ©Ÿèƒ½ TodoCreate
    title: str
    description: Optional[str] = None
    done: bool = False

    å‰Šé™¤æ©Ÿèƒ½ TodoOut
    id: int  # DB ã«ä¿å­˜ã•ã‚ŒãŸ ID ã‚’å«ã‚ã‚‹
"""

# app/services/todo_service.py
from sqlalchemy.orm import Session
from app.models.todo_model import TodoModel
from app.schemas.todo import TodoOut
import os
import json
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate
from fastapi import FastAPI, HTTPException
from fastapi.responses import FileResponse
import os
from dotenv import load_dotenv

# .env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
load_dotenv()


# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
SEARCH_DIR = os.getenv("SEARCH_DIR")  # ã“ã“ã‚’é©åˆ‡ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å¤‰æ›´
api_key = os.getenv("GOOGLE_API_KEY")
llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0)
my_llm_instance = LLMChain(
    llm=llm, 
    prompt=PromptTemplate(
        input_variables=["text"], 
        template="{text}")
)


class ScheduleService:
    def optimize_schedule(self, db: Session, date: str):
        # tasks = db.query(TodoModel).filter(TodoModel.done == False).all()
        tasks = db.query(TodoModel).filter(TodoModel.done.is_(False)).all()

        task_list = [
            {
                "id": t.id,
                "title": t.title,
                "description": t.description,
                "done": t.done,
                "priority": t.priority,
                "estimated_minutes": t.estimated_minutes,
                "time_limit": t.time_limit.isoformat() if t.time_limit else None,
            }
            for t in tasks
        ]

        # LLMã«æ¸¡ã™
        prompt_text = (
            f"ä»Šæ—¥ã®æ—¥ä»˜ã¯ {date} ã§ã™ã€‚\n"
            f"ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’å„ªå…ˆåº¦é †ã« JSON å½¢å¼ã§è¿”ã—ã¦ãã ã•ã„:\n"
            f"{task_list}"
        )
        result = my_llm_instance.run(prompt_text)
        optimized_tasks = json.loads(result)

        # DBã«å„ªå…ˆåº¦ã‚’åæ˜ 
        for idx, task_info in enumerate(optimized_tasks):
            todo = db.query(TodoModel).filter(TodoModel.id == task_info["id"]).first()
            if todo:
                todo.priority = idx
        db.commit()

        # ğŸ”¥ TodoOut ã«å¤‰æ›ã—ã¦è¿”ã™
        return [
            TodoOut(
                id=task["id"],
                title=task["title"],
                description=task.get("description"),
                done=task["done"],
                priority=task.get("priority"),
                estimated_minutes=task.get("estimated_minutes"),
                time_limit=task.get("time_limit"),
            )
            for task in optimized_tasks
        ]


   
    def find_related_file_for_task(task_id: int, db: Session):
        """
        æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¹ã‚¯IDã«é–¢é€£ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’AIãŒæ¢ã—ã€ãã®ãƒ‘ã‚¹ã¨ä¸­èº«ã‚’è¿”ã™ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–¢æ•°
        """
        # 1. ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å–å¾—
        task = db.query(TodoModel).filter(TodoModel.id == task_id).first()
        if not task:
            return None # ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ

        # 2. æ¤œç´¢å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
        files = [os.path.join(root, f) for root, _, fs in os.walk("./docs") for f in fs]
        filenames = [os.path.basename(f) for f in files]

        # 3. LLMã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸ã°ã›ã‚‹ (ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ”¹å–„)
        prompt = f"""
        ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã®ä¸­ã‹ã‚‰ã€
        ã‚¿ã‚¹ã‚¯ã€Œ{task.title}ã€ï¼ˆè©³ç´°ï¼š{task.description}ï¼‰
        ã«æœ€ã‚‚é–¢é€£ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä¸€ã¤ã ã‘é¸ã‚“ã§ã€ãƒ•ã‚¡ã‚¤ãƒ«åã ã‘ã‚’ç­”ãˆã¦ãã ã•ã„ã€‚

        ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ: {filenames}
        """
        # 4. LLMã‚’ä¸€åº¦ã ã‘å®Ÿè¡Œ
        selected_filename = llm.run(prompt).strip()

        # 5. é¸ã°ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã—ã¦ä¸­èº«ã‚’èª­ã¿è¾¼ã‚€ (ã‚ˆã‚Šé ‘å¥ãªæ–¹æ³•)
        for file_path in files:
            if selected_filename in file_path:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                # 6. ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã¨ä¸­èº«ã‚’è¾æ›¸ã§è¿”ã™
                return {"path": file_path, "content": content}

        return None # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ