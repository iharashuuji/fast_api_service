"""
LLMã«æ¨è«–ã‚’ã•ã›ã¦ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æœ€é©åŒ–ã‚’è¡Œã†
å¿…è¦ãªã‚‚ã®ã¯ã€Openai apiã¨ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å–å¾—ã™ã‚‹äº‹ã€ãã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ãã®äººãŒã©ã‚Œãã‚‰ã„ã®ã‚¿ã‚¹ã‚¯ã‚’ã†ã¾ãã•ã°ã‘ã‚‹ã‹ã‚’è€ƒãˆã‚‹ã€‚
ãŸã ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æœ€é©åŒ–ã£ã¦ã€é›£ã—ã„ã‹ãªã¨æ€ã†ãŒã€é–‹ã„ã¦ã„ã‚‹æ™‚é–“ã§ã¡ã‚‡ã£ã¨ã—ãŸèª²é¡Œã¨ã‹ã¯ã‚„ã‚Œã°ã„ã„ãŒã€ãƒ†ã‚¹ãƒˆå‹‰å¼·ï¼ã¨ã‹æ™‚é–“ã‚’ã¾ã¨ã‚ã¦å–ã£ãŸæ–¹ãŒã„ã„ã‚„ã¤ã¨ã‹ã¯ã‹ãªã‚Šã„ã„ã‹ãªã¨æ€ã£ã¦ã„ã‚‹ã€
ãã“ã§ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ä¸­ã§ã€å‹•ãã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã€å›ºå®šã—ã¦æ±ºã‚ã‚‹ã¹ãã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è€ƒãˆã‚‹ã¹ãã‹ãªã¨æ€ã£ãŸã€‚ã“ã®ã‚ãŸã‚Šã®æ•°å€¤ã®èª¿ç¯€ã¯è‡ªåˆ†è‡ªèº«ã§ã‚„ã£ã¦ã‚‚ã‚‰ã†æ–¹ãŒã„ã„ã‹ãªã¨æ€ã†

    ä½œæˆæ©Ÿèƒ½ TodoCreate
    title: str
    description: Optional[str] = None
    done: bool = False

    å‰Šé™¤æ©Ÿèƒ½ TodoOut
    id: int  # DB ã«ä¿å­˜ã•ã‚ŒãŸ ID ã‚’å«ã‚ã‚‹
"""

# app/services/todo_service.py
from sqlalchemy.orm import Session
from app.models.todo_model import TodoModel
from app.schemas.todo import TodoOut
import os
import json
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate

from dotenv import load_dotenv

# .env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
load_dotenv()

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
api_key = os.getenv("GOOGLE_API_KEY")
llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0)
my_llm_instance = LLMChain(
    llm=llm, 
    prompt=PromptTemplate(
        input_variables=["text"], 
        template="{text}")
)


class ScheduleService:
    def optimize_schedule(self, db: Session, date: str):
        # tasks = db.query(TodoModel).filter(TodoModel.done == False).all()
        tasks = db.query(TodoModel).filter(TodoModel.done.is_(False)).all()

        task_list = [
            {
                "id": t.id,
                "title": t.title,
                "description": t.description,
                "done": t.done,
                "priority": t.priority,
                "estimated_minutes": t.estimated_minutes,
                "time_limit": t.time_limit.isoformat() if t.time_limit else None,
            }
            for t in tasks
        ]

        # LLMã«æ¸¡ã™
        prompt_text = (
            f"ä»Šæ—¥ã®æ—¥ä»˜ã¯ {date} ã§ã™ã€‚\n"
            f"ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’å„ªå…ˆåº¦é †ã« JSON å½¢å¼ã§è¿”ã—ã¦ãã ã•ã„:\n"
            f"{task_list}"
        )
        result = my_llm_instance.run(prompt_text)
        optimized_tasks = json.loads(result)

        # DBã«å„ªå…ˆåº¦ã‚’åæ˜ 
        for idx, task_info in enumerate(optimized_tasks):
            todo = db.query(TodoModel).filter(TodoModel.id == task_info["id"]).first()
            if todo:
                todo.priority = idx
        db.commit()

        # ğŸ”¥ TodoOut ã«å¤‰æ›ã—ã¦è¿”ã™
        return [
            TodoOut(
                id=task["id"],
                title=task["title"],
                description=task.get("description"),
                done=task["done"],
                priority=task.get("priority"),
                estimated_minutes=task.get("estimated_minutes"),
                time_limit=task.get("time_limit"),
            )
            for task in optimized_tasks
        ]
